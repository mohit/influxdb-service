reporting-disabled = false

[meta]
  dir = "/data/meta"
  hostname = "localhost"
  bind-address = "0.0.0.0:88"
  retention-autocreate = true
  election-timeout = "1s"
  heartbeat-timeout = "1s"
  leader-lease-timeout = "500ms"
  commit-timeout = "50ms"
  cluster-tracing = false

[data]
  dir = "/data/db"
  max-wal-size = 104857600
  wal-flush-interval = "10m0s"
  wal-partition-flush-delay = "2s"
  wal-dir = "/data/wal"
  wal-enable-logging = true
  wal-ready-series-size = 30720
  wal-compaction-threshold = 0.5
  wal-max-series-size = 1048576
  wal-flush-cold-interval = "5m0s"
  wal-partition-size-threshold = 20971520

[cluster]
  force-remote-mapping = false
  write-timeout = "5s"
  shard-writer-timeout = "5s"
  shard-mapper-timeout = "5s"

[retention]
  enabled = true
  check-interval = "10m0s"

[shard-precreation]
  enabled = true
  check-interval = "10m0s"
  advance-period = "30m0s"

[admin]
  enabled = true
  bind-address = "0.0.0.0:83"
  https-enabled = false
  https-certificate = "/etc/ssl/influxdb.pem"

[http]
  enabled = true
  bind-address = "0.0.0.0:86"
  auth-enabled = false
  log-enabled = true
  write-tracing = false
  pprof-enabled = false
  https-enabled = false
  https-certificate = "/etc/ssl/influxdb.pem"

[[graphite]]
  bind-address = ":2003"
  database = "graphite"
  enabled = false
  protocol = "tcp"
  batch-size = 1000
  batch-timeout = "1s"
  consistency-level = "one"
  separator = "."

[collectd]
  enabled = false
<<<<<<< HEAD
  bind-address = ":25826"
  database = "collectd"
  retention-policy = ""
  batch-size = 5000
  batch-timeout = "10s"
  typesdb = "/usr/share/collectd/types.db"

[opentsdb]
  enabled = false
  bind-address = ":4242"
  database = "opentsdb"
  retention-policy = ""
  consistency-level = "one"
  tls-enabled = false
  certificate = "/etc/ssl/influxdb.pem"

[monitoring]
  enabled = false
  write-interval = "1m0s"

[continuous_queries]
  log-enabled = true
  enabled = true
  recompute-previous-n = 2
  recompute-no-older-than = "10m0s"
  compute-runs-per-interval = 10
  compute-no-more-than = "2m0s"

[hinted-handoff]
  enabled = true
  dir = "/data/hh"
  max-size = 1073741824
  max-age = "168h0m0s"
  retry-rate-limit = 0
  retry-interval = "1s"
=======
  # port = 4444
  # database = ""

# Raft configuration
[raft]
# The raft port should be open between all servers in a cluster.
# However, this port shouldn't be accessible from the internet.

port = 90

# Where the raft logs are stored. The user running InfluxDB will need read/write access.
dir  = "/data/raft"

# election-timeout = "1s"

[storage]
dir = "/data/db"
# How many requests to potentially buffer in memory. If the buffer gets filled then writes
# will still be logged and once the local storage has caught up (or compacted) the writes
# will be replayed from the WAL
write-buffer-size = 10000

[cluster]
# A comma separated list of servers to seed
# this server. this is only relevant when the
# server is joining a new cluster. Otherwise
# the server will use the list of known servers
# prior to shutting down. Any server can be pointed to
# as a seed. It will find the Raft leader automatically.

# Here's an example. Note that the port on the host is the same as the raft port.
# seed-servers = ["influxdb.ops.clever.com:8090"]

# Replication happens over a TCP connection with a Protobuf protocol.
# This port should be reachable between all servers in a cluster.
# However, this port shouldn't be accessible from the internet.

protobuf_port = 99
protobuf_timeout = "2s" # the write timeout on the protobuf conn any duration parseable by time.ParseDuration
protobuf_heartbeat = "200ms" # the heartbeat interval between the servers. must be parseable by time.ParseDuration
protobuf_min_backoff = "1s" # the minimum backoff after a failed heartbeat attempt
protobuf_max_backoff = "10s" # the maxmimum backoff after a failed heartbeat attempt

# How many write requests to potentially buffer in memory per server. If the buffer gets filled then writes
# will still be logged and once the server has caught up (or come back online) the writes
# will be replayed from the WAL
write-buffer-size = 10000

# the maximum number of responses to buffer from remote nodes, if the
# expected number of responses exceed this number then querying will
# happen sequentially and the buffer size will be limited to this
# number
max-response-buffer-size = 100

# When queries get distributed out to shards, they go in parallel. This means that results can get buffered
# in memory since results will come in any order, but have to be processed in the correct time order.
# Setting this higher will give better performance, but you'll need more memory. Setting this to 1 will ensure
# that you don't need to buffer in memory, but you won't get the best performance.
concurrent-shard-query-limit = 10

[leveldb]

# Maximum mmap open files, this will affect the virtual memory used by
# the process
max-open-files = 40

# LRU cache size, LRU is used by leveldb to store contents of the
# uncompressed sstables. You can use `m` or `g` prefix for megabytes
# and gigabytes, respectively.
lru-cache-size = "200m"

# The default setting on this is 0, which means unlimited. Set this to something if you want to
# limit the max number of open files. max-open-files is per shard so this * that will be max.
max-open-shards = 0

# The default setting is 100. This option tells how many points will be fetched from LevelDb before
# they get flushed into backend.
point-batch-size = 100

# The number of points to batch in memory before writing them to leveldb. Lowering this number will
# reduce the memory usage, but will result in slower writes.
write-batch-size = 5000000

# These options specify how data is sharded across the cluster. There are two
# shard configurations that have the same knobs: short term and long term.
# Any series that begins with a capital letter like Exceptions will be written
# into the long term storage. Any series beginning with a lower case letter
# like exceptions will be written into short term. The idea being that you
# can write high precision data into short term and drop it after a couple
# of days. Meanwhile, continuous queries can run downsampling on the short term
# data and write into the long term area.
[sharding]
  # how many servers in the cluster should have a copy of each shard.
  # this will give you high availability and scalability on queries
  replication-factor = 1

  [sharding.short-term]
  # each shard will have this period of time. Note that it's best to have
  # group by time() intervals on all queries be < than this setting. If they are
  # then the aggregate is calculated locally. Otherwise, all that data gets sent
  # over the network when doing a query.
  duration = "7d"

  # split will determine how many shards to split each duration into. For example,
  # if we created a shard for 2014-02-10 and split was set to 2. Then two shards
  # would be created that have the data for 2014-02-10. By default, data will
  # be split into those two shards deterministically by hashing the (database, serise)
  # tuple. That means that data for a given series will be written to a single shard
  # making querying efficient. That can be overridden with the next option.
  split = 1

  # You can override the split behavior to have the data for series that match a
  # given regex be randomly distributed across the shards for a given interval.
  # You can use this if you have a hot spot for a given time series writing more
  # data than a single server can handle. Most people won't have to resort to this
  # option. Also note that using this option means that queries will have to send
  # all data over the network so they won't be as efficient.
  # split-random = "/^hf.*/"

  [sharding.long-term]
  duration = "30d"
  split = 1
  # split-random = "/^Hf.*/"

[wal]

dir   = "/data/wal"
flush-after = 1000 # the number of writes after which wal will be flushed, 0 for flushing on every write
bookmark-after = 1000 # the number of writes after which a bookmark will be created

# the number of writes after which an index entry is created pointing
# to the offset of the first request, default to 1k
index-after = 1000

